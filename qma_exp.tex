\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,lmargin=1in,rmargin=1in}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{definition}
  \newtheorem{defn}[thm]{\protect\definitionname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}
  \theoremstyle{definition}
  \newtheorem{problem}[thm]{\protect\problemname}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{\protect\remarkname}
  \theoremstyle{remark}
  \newtheorem*{rem*}{\protect\remarkname}
  \theoremstyle{plain}
  \newtheorem{algorithm}[thm]{\protect\algorithmname}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{tikz} 

%\input{Qcircuit}
\input{macros}
\makeatother

\usepackage{babel}
  \providecommand{\algorithmname}{Algorithm}
  \providecommand{\definitionname}{Definition}
  \providecommand{\lemmaname}{Lemma}
  \providecommand{\problemname}{Problem}
  \providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\newcommand{\Expect}{{\rm I\kern-.3em E}}

\begin{document}

\title{The complexity of QMA with inverse exponential gap}
\author{Bill Fefferman, Cedric Lin}
\date{\today}
\maketitle
%\begin{abstract}
%
%\end{abstract}

We will study the complexity of QMA proof systems with inverse exponential gap. Define $\text{QMA}(c,s)$ to be QMA with completeness $c$ and soundness $s$, and write $\text{QMA}_{exp} := \cup_{c}\text{QMA}(c,c-2^{-\text{poly}})$. Our goal  will be to study the relationship between $\text{QMA}_{exp}$ and PSPACE.

\begin{con}
$\text{\emph{QMA}}_{exp}=\text{\emph{PSPACE}}$.
\end{con}

It is already known that
\begin{thm}[Ito, Kobayashi, and Watrous, Theorem 11] 
$\text{\emph{QMA}}(1,<1)\subseteq\text{\emph{PSPACE}}$.
\end{thm}
%Some other useful results:
%\begin{thm}[Watrous]
%$\text{\emph{PQP}} = \text{\emph{PP}}$, where \emph{PQP} is the unbounded-gap version of \emph{BQP}.
%\end{thm}
%\begin{thm}
%$\text{\emph{PQPSPACE}} = \text{\emph{PPSPACE}} = \text{\emph{PSPACE}} $.
%\end{thm}
%\begin{thm}[Aaronson]
%$\text{\emph{PostBQP}} = \text{\emph{PP}}$.
%\end{thm}
Another useful result:
\begin{thm}[Watrous]
$\text{\emph{PQPSPACE}} = \text{\emph{PPSPACE}} = \text{\emph{PSPACE}} $.
\end{thm}

\section{$\text{QMA}_{exp} \subseteq \text{PSPACE}$}
In this section we will sketch a proof of the following result:
\begin{thm}
$\text{\emph{QMA}}_{exp} \subseteq \text{\emph{PSPACE}}$.
\end{thm}
\begin{proof}
Let $L = (L_{yes},L_{no})$ be a promise problem in $\text{QMA}_m(c,s)$ with $c - s = 2^{-\text{poly}}$, where $m \in \text{poly}$ indicates the number of qubits in the witness. Suppose the collection of circuits $\{C'_x\}_x$ is a verification procedure for $L$; that is, each circuit $C'_x$ acts on $k+m$ qubits for some $k \in \text{poly}$, and if $x \in L_{yes}$ there exists an $m$-qubit state $\ket{\psi}$ such that
\begin{equation}
\L(\bra{\psi}\otimes \bra{0^k}\R) C'^\dagger_x \ket{1}\bra{1}_{out} C'_x \L(\ket{\psi}\otimes \ket{0^k}\R) \ge c
\end{equation}
whereas if $x \in L_{no}$, for all $m$-qubit states $\ket{\psi}$ we have
\begin{equation}
\L(\bra{\psi}\otimes \bra{0^k}\R) C'^\dagger_x \ket{1}\bra{1}_{out} C'_x \L(\ket{\psi}\otimes \ket{0^k}\R) \le s.
\end{equation}

We will use the following steps to decide which is the case in PSPACE.

\subsection{Gap Amplification}

Define the projectors
\begin{align}
\Pi_0 &= I_m \otimes \ket{0^k}\bra{0^k} \\
\Pi_1 &= C'^\dagger_x \L(\ket{1}\bra{1}_{out} \otimes I_{m+k-1}\R) C'_x
\end{align}
and the reflections
\begin{align}
R_0 &= 2\Pi_0 - I \\
R_1 &= 2\Pi_1 - I.
\end{align}
Note that we are asked to decide whether there exists $\ket{\psi}$ satisfying $\L(\bra{\psi}\otimes \bra{0^k}\R) \Pi_1 \L(\ket{\psi}\otimes \ket{0^k}\R) \ge c$, or that all states $\ket{\psi}$ satisfy $\L(\bra{\psi}\otimes \bra{0^k}\R) \Pi_1 \L(\ket{\psi}\otimes \ket{0^k}\R) \ge s$.
According to the fast QMA amplification procedure of Nagaj, Wocjan, and Zhang [arXiv:0904.1549], the following procedure will determine which is the case, with completeness at least $1-2^{-r}$ and soundness at most $2^{-r}$:

\begin{enumerate}
\item Perform $r$ trials of phase estimation of the operator $R_1R_0$ on the state $\ket{\psi}\otimes \ket{0^k}$, with $O(\log(c-s)) = O(\text{poly})$ bits of precision and $1/16$ failure probability. 
\item If the median of the $r$ results is at most $\phi_c = \arccos\sqrt{c}/\pi$, output YES; otherwise if the result is at least $\phi_s = \arccos\sqrt{s}/\pi$, output NO.
\end{enumerate}

Phase estimation of an operator $U$ up to $a$ bits of precision requires $O(a)$ ancilla qubits and $O(2^a)$ applications of the control-$U$ operation. Therefore this gap amplification procedure, for any polynomial $r$, requires an exponential number of gates to implement, but only a polynomial number of extra ancilla qubits.

We therefore see that we can obtain a (possibly exponentially long) circuit $C_x$, acting on $m+p$ qubits for some $p \in \text{poly}$, such that if $x \in L_{yes}$ there exists an $m$-qubit state $\ket{\psi}$ such that
\begin{equation}
\L(\bra{\psi}\otimes \bra{0^p}\R) C^\dagger_x \ket{1}\bra{1}_{out} C_x \L(\ket{\psi}\otimes \ket{0^p}\R) \ge 1-2^{-(m+2)}
\end{equation}
whereas if $x \in L_{no}$, for all $m$-qubit states $\ket{\psi}$ we have
\begin{equation}
\L(\bra{\psi}\otimes \bra{0^p}\R) C^\dagger_x \ket{1}\bra{1}_{out} C_x \L(\ket{\psi}\otimes \ket{0^p}\R) \le 2^{-(m+2)}.
\end{equation}
For convenience, define the $2^m \times 2^m$ matrix
\begin{equation}
Q_x := \L(I_{2^m}\otimes \bra{0^p}\R) C^\dagger_x \ket{1}\bra{1}_{out} C_x \L(I_{2^m}\otimes \ket{0^p}\R).
\end{equation}
Note that $Q_x$ is positive semidefinite, and $\bra{\psi}Q_x\ket{\psi}$ is the acceptance probability of our procedure on witness $\psi$ (at least $1-2^{-r}$ in the YES case, at most $2^{-r}$ in the NO case).

We use the following reduction from Marriott and Watrous: note that in the YES case $\tr[Q_x]\ge 1 - 2^{-m/2} \ge 3/4$, since the trace is at least the largest eigenvalue; while in the NO case $\tr[Q_x]\le 2^m \cdot 2^{-m/2} =1/4$ since the trace is the sum of the $2^m$ eigenvalues, each of which is at most $2^{-m/2}$. Therefore our problem reduces to determining whether the trace of $Q_x$ is at least $3/4$ or at most $1/4$.

We now see two different ways of proceeding from here, which we will detail in two separate subsections.

\subsection{Directly applying the $\text{PQPSPACE=PSPACE}$ relation}
We use the following idea, already present in the proof of Theorem 3.6 of Marriott and Watrous. Simply use the totally mixed state (alternatively, a random computational basis state) as the witness of the verification procedure encoded by $Q_x$. The totally mixed state is $2^{-m}I_m$, and therefore the acceptance probability is given by
\begin{equation}
\tr(Q_x 2^{-m}I_m) = 2^{-m} \tr(Q_x)
\end{equation}
which is at least $2^{-m} \cdot 3/4$ for the YES case, and at most $2^{-m} \cdot 1/4$ for the NO case. Thus we have reduced our original problem to determining whether an exponentially long quantum computation with \emph{no} witness, acting on a polynomial number of qubits, accepts with probability at least $c'$ or at most $s'$ with $c' - s'$ being exponentially small. This is a PQPSPACE (unbounded-error probabilitistic quantum polynomial space) problem; using Watrous's result that $\text{PQPSPACE=PSPACE}$ we immediately get that this is a PSPACE problem as well. Therefore $\text{QMA}_{exp} \subseteq \text{PSPACE}$, as claimed.

\subsection{Using path integrals}
We now present the other approach, which was our original idea. 

Write $\Delta_1 = \ket{1}\bra{1}_{out}$; then we want to calculate the quantity
\begin{equation}
\tr[Q_x] = \sum_{s_0 \in [2^m] \times \{0^p\}} \bra{s_0} C^\dagger_x \Delta_1 C_x \ket{s_0}.
\end{equation}
Let $C = U_TU_{T-1}\cdots U_1$ for single-qubit or two-qubit unitaries $U_i$; then
\begin{equation}
\tr[Q_x] = \sum_{s_0 \in [2^m] \times \{0^p\}} \bra{s_0} U_1^\dagger \cdots U_T^\dagger \Delta_1 U_T\cdots U_1 \ket{s_0}.
\end{equation}
Inserting complete sets of states $\ket{s'_1},\cdots,\ket{s'_T},\ket{s_T},\cdots,\ket{s_1}$ where $s'_i,s_i \in [2^{m+p}]$ we obtain
\begin{equation}
\tr[Q_x] = \sum_{s_0 \in [2^m] \times \{0^p\}} \sum_{s'_i,s_i \in [2^{m+p}]} \bra{s_0} U_1^\dagger \ket{s'_1}\bra{s'_1}U_2^\dagger\ket{s'_2} \cdots \bra{s'_{T-1}}U_T^\dagger\ket{s'_T} \bra{s'_T}\Delta_1 \ket{s_T}\bra{s_T}U_T\ket{s_{T-1}}\cdots \bra{s_1}U_1 \ket{s_0}.
\end{equation}
Since $T$ could be exponentially large, we see that $\tr[Q_x]$ is a sum over doubly exponentially many summands, each of which is a product over exponentially many factors. Nevertheless, by following the path integral approach of Fortnow and Rogers it should be possible to turn the calculation of $\tr[Q_x]$ into a GapPSPACE problem, and therefore determining whether $\tr[Q_x] \ge c'$ or $\le s'$ into a $\text{PPSPACE} = \text{PSPACE}$ problem. It should be clear how to do this if all the transition amplitudes in the unitaries $U_i$ are rational; the general case (with algebraic amplitudes) will take more work.

\end{proof}

\section{Lower bound}
We now discuss lower bounds for $\text{QMA}_{exp}$. The containment $\text{PP}\subseteq \text{QMA}_{exp}$ is fairly obvious: the completeness-soundness gap is at least inverse exponential for PP, and giving the verifier quantum power as well as a potential witness can only increase its power.

We will show something stronger:
\begin{thm}
$ \text{\emph{PSPACE}} \subseteq \text{\emph{QMA}}_{exp}$.
\end{thm}

\begin{proof}

Consider the problem of determining whether an exponentially-large, succinctly representable, sparse symmetric matrix is singular; we will show that this problem is PSPACE-complete. Note then that determining whether a sparse matrix $A$ is singular is equivalent to determining whether $A$ has 0 as an eigenvalue; we will show that the latter can be solved in $\text{QMA}_{exp}$.

By a matrix being succinctly representable and sparse, we mean the following:
\begin{defn}
Let $M$ be a $2^{\text{poly}(n)} \times 2^{\text{poly}(n)}$ matrix, where $n$ is the input size. We say that $M$ is a \emph{succinctly representable sparse} matrix if there are at most polynomially many nonzero entries in each row, and moreover there is a (uniformly generated) circuit to output the nonzero entries of any given row in $\text{poly}(n)$ time.
\end{defn}

\subsection{Sparse matrix singularity is PSPACE-complete}

We first state the following result of [arXiv:1210.1451]:
\begin{lem}
Let $A'$ be an exponentially-large, succinctly representable sparse matrix, whose determinant is promised to be 0, 1, or -1. Moreover, each column of $A'$ has at most two 1s. It is PSPACE-complete to determine if the determinant of $A'$ vanishes.
\end{lem}

We proceed to reproduce the proof of [arXiv:1210.1451] of this result, as we will need to modify it for our purposes.

\begin{proof}
Csanky's algorithm immediately gives that this problem is in PSPACE, and therefore we only need to show PSPACE-hardness.

Let $L=(L_{yes},L_{no}) \in \text{PSPACE}$ be decided by a deterministic Turing Machine $M$ in polynomial space. Consider the configuration graph $G^M$ of the matrix $M$: each vertex of $G_M$ corresponds to one of the exponentially many configurations of $M$, each of which is describable with polynomially many bits. The configuration graph $G^M$ has an edge from $c$ to $c'$ if and only if $c'$ can be reached from $c$ in one step of computation. It is straightforward to see that $G^M$ has the following properties:
\begin{itemize}
\item Since $M$ is deterministic, all vertices of $G^M$ have out-degree at most 1, and $G_M$ has no cycles.
\item The adjacency matrix of $G_M$ is a succinctly representable sparse matrix.
\item $M$ accepts input $x$ if and only if there is a path in $G^M$ from the starting configuration $s_x$ to the accepting configuration $t$.
\end{itemize}

Now on input $x$, consider the graph $G^M_x$ obtained by adding an edge from the accepting configuration $t$ to the starting configuration $s_x$, and adding self-loops on all other vertices. Let $A^M_x$ be the adjacency matrix of $G^M_x$. It then turns out that $\det(A^M_x) = \pm 1$ if and only if there is a path from $s_x$ to $t$, i.e. $M$ accepts input $x$; otherwise $\det(A^M_x) = 0$. Therefore computing $\det(A^M_x)$ is PSPACE-hard.
\end{proof}
We can immediately see the following:
\begin{cor}
Let $A$ be an exponentially-large, succinctly representable sparse matrix, whose determinant is promised to be 0, or 1. Moreover, $A$ is symmetric and positive semidefinite. It is PSPACE-complete to determine if the determinant of $A$ vanishes.
\end{cor}
\begin{proof}
Note that the matrix $(A^M_x)^T A^M_x$ is a succinctly representable sparse matrix, because there at most two 1s in each column of $A^M_x$; and deciding whether $\det((A^M_x)^T A^M_x) = \det(A^M_x)^2$ vanishes is PSPACE-hard.
\end{proof}

From here, we would like to argue that given a succinctly representable sparse and symmetric matrix $A$, it is PSPACE-complete to distinguish to determine whether $\det(A) = 0$ or $|\det(A)| > 2^{-\text{poly}}$, promised that one of this is the case. We will see later that this problem can be solved in $\text{QMA}_{exp}$. Unfortunately, this promise does not generally hold for succinctly representable sparse symmetric matrices; at worse if $A$ is nonsingular the smallest eigenvalue can still be doubly exponentially small. We will therefore need to modify the PSPACE-complete construction a bit.

First of all, we make the following definition:
\begin{defn}
A deterministic Turing machine $M$ is reversible if all vertices of the configuration graph $G^M$ have in-degree at most 1.
\end{defn}
Since we already know vertices of $G^M$ have out-degree at most 1, this implies that $G^M$ is simply a collection of disjoint paths.
\begin{thm}[Lange et al.]
Every bijective function computable by a deterministic Turing machine in space $S(n)$ can be computed by a reversible deterministic Turing machine in the same space $S(n)$.
\end{thm}
This is good news for us: it means it suffices to examine the case where $G^M$ is a collection of disjoint paths, and so the calculation of the eigenvalues of $G^M_x$ will be much simpler. In fact, [Lange] shows we can even assume the starting configuration $s_x$ has in-degree 0, as long as the input $x$ is kept on the tape at the end; the accepting configuration $t_x$ will therefore depend on $x$. 

Therefore if $\det(A^M_x) \neq 0$, i.e. if $M$ accepts $x$, there is a maximal path in $G^M$ starting from $s_x$ and ending at $t_x$. Assume the path has $\ell+1$ vertices, where $\ell = 2^{O(\text{poly})}$. $G^M_x$ adds an edge from $t_x$ to $s_x$ and adds a self-loop to all other vertices. Therefore if we reorder vertices and remove vertices not connected to $s_x$ and $t_x$, the relevant block of adjacency matrix $A^M_x$ is the $(\ell+1) \times (\ell+1)$ matrix
\begin{equation}
A'^M_x = 
\begin{bmatrix}
    0 & 0 & 0 & 0 & \dots  & 0  & 1 \\
    1 & 1 & 0 & 0 &\dots  & 0 & 0 \\
    0 & 1 & 1 & 0 & \dots  & 0 & 0 \\
     0 & 0 & 1 & 1 & \dots  & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \dots  & 1 & 0 \\
    0 & 0 & 0 & 0 & \dots  & 1 & 0
\end{bmatrix}
\end{equation}
We can directly evaluate $(A'^M_x)^T A'^M_x$, obtaining the following matrix:
\begin{equation}
(A'^M_x)^T A'^M_x = 
\begin{bmatrix}
    1 & 1 & 0 & 0 & \dots  & 0 & 0  & 0 \\
    1 & 2 & 1 & 0 &\dots  & 0 & 0 & 0 \\
    0 & 1 & 2 & 1 & \dots  & 0 & 0 & 0 \\
     0 & 0 & 1 & 2 & \dots  & 0 & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & \dots  & 2 & 1 & 0 \\
    0 & 0 & 0 & 0 & \dots  & 1 & 2 & 0 \\
    0 & 0 & 0 & 0 & \dots  & 0 & 0 & 1
\end{bmatrix}
\end{equation}
For purposes of calculating the smallest eigenvalue the last row and column can be ignored, leaving an $\ell \times \ell$ matrix. The characteristic equation for the eigenvalues $\lambda$ is then $p_\ell(\lambda) = 0$, where
\begin{equation}
p_\ell(\lambda) := 
\det \L(
\begin{bmatrix}
    1 - \lambda & 1 & 0 & 0 & \dots  & 0 & 0  \\
    1 & 2 - \lambda & 1 & 0 &\dots  & 0 & 0 \\
    0 & 1 & 2 - \lambda & 1 & \dots  & 0 & 0 \\
     0 & 0 & 1 & 2 - \lambda & \dots  & 0 & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & 0 & \dots  & 2 - \lambda & 1 \\
    0 & 0 & 0 & 0 & \dots  & 1 & 2 - \lambda \\
\end{bmatrix} \R).
\end{equation}
Now define the polynomial $q_n(x)$ to be the following $n \times n$ determinant:
\begin{equation}
q_n(x) := 
\det \L(
\begin{bmatrix}
    x & 1 & 0 & \dots  & 0 & 0  \\
    1 & x & 1 &\dots  & 0 & 0 \\
    0 & 1 & x  & \dots  & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & x & 1 \\
    0 & 0 & 0 & \dots  & 1 & x \\
\end{bmatrix} \R)
\end{equation}
Note that $p_\ell(\lambda) = q_\ell(2-\lambda) - q_{\ell-1}(2-\lambda)$. Moreover, $q_n(x)$ satisfies the recurrence
\begin{equation}
q_0(x) = 1, \quad q_1(x) = x, \quad q_n(x) = x q_{n-1}(x) - q_{n-2} (x)
\end{equation}
This is the same recurrence satisfied by $U_n(x/2)$, where $U_n(x)$ is the $n$-th degree Chebyshev polynomial of the second kind. Therefore $q_n(x) = U_n(x/2)$. Moreover, using $U_n(\cos\theta) = \sin((n+1)\theta)/\sin(\theta)$ we can evaluate
\begin{align}
q_\ell(x) - q_{\ell}(x) &= \frac{\sin((\ell+1)\theta) - \sin(\ell \theta)}{\sin\theta} \\
&= \frac{\sin((2\ell+1)\theta/2)}{\sin (\theta/2)}
\end{align}
and therefore the zeroes of $q_\ell(x) - q_{\ell}(x)$ are $2\cos\L(\frac{2k}{2\ell + 1}\pi\R)$, $k = 1,\cdots,\ell$. The zeroes of the polynomial $p_\ell(\lambda) = q_\ell(2-\lambda) - q_{\ell-1}(2-\lambda)$ are therefore 
\begin{equation}
\lambda_k = 2\L(1 - \cos\L(\frac{2k}{2\ell+1}\pi\R)\R)
\end{equation}
and the smallest eigenvalue $\lambda_1 = \Theta(\ell^{-2})$ is inverse exponentially bounded away from zero, because $\ell = 2^{O(\text{poly})}$.

\subsection{Sparse matrix singularity is in $\text{QMA}_{exp}$}
We will now give a $\text{QMA}_{exp}$ algorithm for testing if the determinant of a succinctly representable sparse matrix $M$ vanishes, i.e. if 0 is an eigenvalue of $M$. First note the following:
\begin{rem}
Suppose a 0-1 matrix $M$ has at most $d$ nonzero entries per row. Then $\| M \| \le d$.
\end{rem}


\end{proof}

\end{document}
